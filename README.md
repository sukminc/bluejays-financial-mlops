# âš¾ Blue Jays Moneyball: Data Qualityâ€“Driven ETL Platform

![Python](https://img.shields.io/badge/Python-3.12%2B-blue)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.10%2B-orange)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Data%20Warehouse-blue)
![Pipeline Status](https://img.shields.io/badge/Pipeline-Production%20Ready-success)
![Test Coverage](https://img.shields.io/badge/Regression%20Suite-Passing-brightgreen)

## ðŸ“Œ Project Overview

This is a **Production-Grade Data Engineering Platform** designed to ingest, validate, and transform MLB player financial data.

Unlike typical analytics projects that focus solely on "getting data," this project prioritizes **Data Integrity, Auditability, and System Reliability**. It demonstrates a **Senior SDET / Data QA** approach to building pipelines: **"If the data quality cannot be guaranteed, the pipeline must fail."**

---

## ðŸ”„ The Architectural Pivot: Reliability > Complexity

### ðŸš« Phase 1: The Trap of Over-Engineering
Initially, this project utilized a complex **Playwright & Selenium** web scraper to fetch data from Spotrac.
* **The Issue:** DOM structure changes caused frequent silent failures. The cost of maintenance outweighed the value of automation.
* **The Decision:** I deprecated the scraper in favor of **Manual Ingestion with Strict Validation**.

### âœ… Phase 2: The v2 Robust Architecture
I refactored the system into a **"Fail-Fast" ELT Pipeline**:
1.  **Ingestion:** Robust loading of raw CSVs into a `TEXT`-based Staging Layer (No type errors on load).
2.  **Guardrails:** A SQL-based **DQ Gate** that blocks downstream processing if data violates core business rules (Nulls, Duplicates, Invalid Formats).
3.  **Transformation:** A cleaned `Fact` table is created only *after* quality checks pass.
4.  **Regression Testing:** A dedicated CI/CD DAG that tests the pipeline itself against known "Bad Data" to ensure the safety mechanisms are working.

---

## ðŸ— System Architecture

### 1ï¸âƒ£ The Production Pipeline (`bluejays_v2_simple_pipeline`)

```mermaid
graph TD
    A[Source: Spotrac CSV] -->|ingest_raw.py| B[Staging: Raw Text Table]
    B --> C{DQ Gate}
    C -->|âŒ Fail| D[BLOCK PIPELINE]
    C -->|âœ… Pass| E[Transformation Layer]
    E -->|Clean & Type Cast| F[(Fact Table: fact_salary)]

```

### 2ï¸âƒ£ The Regression Suite (`bluejays_regression_suite`)

*Runs on deployment to verify that DQ logic catches errors.*

```mermaid
graph TD
    subgraph Positive Test
    A[Load Clean Data] --> B{Verify DQ Passes}
    B -->|Success| C[âœ… Test Passed]
    end
    
    subgraph Negative Test
    D[Load Bad Data] --> E{Verify DQ Fails}
    E -->|Caught Error| F[âœ… Test Passed]
    end

```

---

## ðŸ›¡ï¸ Key Features (The SDET Approach)

### 1. Robust Raw Ingestion

We load all data as `TEXT` first.

* **Why?** Loading `$10,000` directly into an `INTEGER` column causes ingestion failures.
* **Solution:** Load as text -> Audit -> Clean -> Cast. This ensures we never lose source data.

### 2. The Data Quality Gate (Fail-Fast)

Before any transformation, we run SQL checks located in `src/dq/sql/`.

* **Null Integrity:** Ensures key fields (Player Name, Season) are present.
* **Uniqueness:** Prevents duplicate salary entries.
* **Type Validation:** Uses Regex (`!~ '^\$?[0-9,]+'`) to detect garbage strings like "Unknown" or "TBD" in numeric fields.

### 3. The Regression Suite (CI/CD)

How do we know our tests work? **We test the tests.**

* The `bluejays_regression_suite` DAG loads a purely malicious dataset (`spotrac_bad_data.csv`).
* It asserts that the pipeline **fails**.
* If the pipeline accidentally succeeds on bad data, the Regression Suite fails, alerting the engineer that a guardrail is broken.

---

## ðŸ“‚ Project Structure

```text
bluejays-financial-mlops/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ bluejays_simple_dag.py        # ðŸš€ Prod: Ingest -> DQ -> Transform
â”‚   â””â”€â”€ bluejays_regression_suite.py  # ðŸ§ª QA: Meta-testing the pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ models.py                 # SQLAlchemy Schemas (Staging & Fact)
â”‚   â”œâ”€â”€ dq/
â”‚   â”‚   â”œâ”€â”€ checks.py                 # The DQ Gate Logic
â”‚   â”‚   â””â”€â”€ sql/                      # SQL Validation Rules
â”‚   â”‚       â”œâ”€â”€ check_nulls.sql
â”‚   â”‚       â”œâ”€â”€ check_duplicates.sql
â”‚   â”‚       â””â”€â”€ check_invalid_types.sql
â”‚   â””â”€â”€ load/
â”‚       â”œâ”€â”€ ingest_raw.py             # Universal CSV Loader (Prod/Test modes)
â”‚       â””â”€â”€ transform_fact_salary.py  # ELT Logic (Text -> Number)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/manual/
â”‚       â”œâ”€â”€ spotrac_bluejays_2025.csv # âœ… Golden Data
â”‚       â””â”€â”€ spotrac_bad_data.csv      # âŒ Test Data (Chaos Engineering)
â””â”€â”€ docker-compose.yaml

```

---

## âš™ï¸ How to Run

### 1. Build & Start

```bash
docker-compose build
docker-compose up -d

```

### 2. Run the Production Pipeline

1. Go to `http://localhost:8080` (User/Pass: `airflow`).
2. Trigger **`bluejays_v2_simple_pipeline`**.
3. Observe: Ingest (Success) â†’ DQ (Success) â†’ Transform (Success).

### 3. Run the Regression Test

1. Trigger **`bluejays_regression_suite`**.
2. Observe:
* **Task 1 (Clean Data):** Passes.
* **Task 2 (Bad Data):** The script detects the bad data, asserts failure, and returns **Success** (Green) because the system worked as designed.



---

## ðŸ‘¨â€ðŸ’» About the Author

**Chris (Suk Min) Yoon**
*Senior SDET / Data QA Engineer (10+ Years Experience)*

Specializing in **ETL Validation**, **Data Integrity**, and **Automation-First Quality Systems**.

> **Portfolio Note:** This project demonstrates the discipline to prioritize system stability. By implementing a Regression Suite for the data pipeline itself, I ensure that data quality rules are treated as first-class code citizens that must pass CI/CD.
